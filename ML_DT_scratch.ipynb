{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Creating a machine learning decision tree from scratch"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "#Importing numpy and pandas\nimport numpy as np\nimport pandas as pd"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2022-10-03 23:47:33--  https://ml-decision-tree-data.s3.amazonaws.com/data.csv\nResolving ml-decision-tree-data.s3.amazonaws.com (ml-decision-tree-data.s3.amazonaws.com)... 52.217.169.57\nConnecting to ml-decision-tree-data.s3.amazonaws.com (ml-decision-tree-data.s3.amazonaws.com)|52.217.169.57|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2742 (2.7K) [text/csv]\nSaving to: \u2018data.csv\u2019\n\ndata.csv            100%[===================>]   2.68K  --.-KB/s    in 0s      \n\n2022-10-03 23:47:34 (31.8 MB/s) - \u2018data.csv\u2019 saved [2742/2742]\n\n"
                }
            ],
            "source": "#Accesing the data\n!wget -O data.csv https://ml-decision-tree-data.s3.amazonaws.com/data.csv"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature1</th>\n      <th>feature2</th>\n      <th>feature3</th>\n      <th>feature4</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.0</td>\n      <td>3.5</td>\n      <td>1.3</td>\n      <td>0.3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.9</td>\n      <td>3.1</td>\n      <td>4.9</td>\n      <td>1.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.8</td>\n      <td>2.6</td>\n      <td>4.0</td>\n      <td>1.2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.1</td>\n      <td>3.3</td>\n      <td>1.7</td>\n      <td>0.5</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   feature1  feature2  feature3  feature4  class\n0       5.0       3.5       1.3       0.3      0\n1       6.9       3.1       4.9       1.5      1\n2       5.8       2.6       4.0       1.2      1\n3       6.7       3.0       5.2       2.3      2\n4       5.1       3.3       1.7       0.5      0"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#Reading the data to a dataframe\ndata = pd.read_csv('data.csv')\ndata.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "class Node:\n    \n    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n        # Start code here\n        # for decision node\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.info_gain = info_gain\n        \n        # for leaf node\n        self.value = value\n    # End code here"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "class DecisionTreeClassifier:\n    def __init__(self, min_samples_split=2, max_depth=2):\n        # Initialize the root of the decision tree to traverse through the decision tree to None\n        self.root = None\n        # initialize the stopping conditions\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n\n    def build_tree(self, dataset, curr_depth=0):\n        \"\"\"\n        This will be a recursive function to build the decision tree.\n        dataset: The data that you will be using for your assignment\n        curr_depth: Current depth of the tree\n        Returns the leaf node\n        \"\"\"\n        # Separate the features and targets into two variables X and Y\n        X = dataset[:,:-1]\n        Y = dataset[:,-1]\n        \n        # Extract the number of samples and number of features\n        num_samples, num_features = np.shape(X)\n        \n        # split until stopping conditions are met\n        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n            best_split = self.get_best_split(dataset, num_samples, num_features)\n            if best_split[\"info_gain\"]>0:\n                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n                \n                return Node(best_split[\"feature_index\"],  \n                            best_split[\"threshold\"], \n                            left_subtree, right_subtree,\n                            best_split[\"info_gain\"])\n        \n        # compute leaf node\n        leaf_value = self.calculate_leaf_value(Y)\n        \n        # return the leaf node\n        return Node(value=leaf_value)\n\n    def get_best_split(self, dataset, num_samples, num_features):\n        \"\"\"\n        Function to find out the best split\n        dataset: input data\n        num_samples: Number of samples present in the dataset\n        num_features: Number of features in the dataset\n        Returns the best split\n        \"\"\"\n\n        # dictionary to store the best split\n        best_split = {}\n        max_info_gain = -float(\"inf\")\n        \n        # loop over all the features in the data\n        for feature_index in range(num_features):\n            feature_values = dataset[:, feature_index]\n            possible_thresholds = np.unique(feature_values)\n            for threshold in possible_thresholds:\n                dataset_left ,dataset_right = self.split(dataset, feature_index, threshold)\n                if len(dataset_left)>0 and len(dataset_right)>0:\n                    y, left_y, right_y = dataset[:,-1], dataset_left[:,-1], dataset_right[:,-1]\n                    curr_info_gain = self.information_gain(y, left_y, right_y, \"entropy\")\n                    \n                    if curr_info_gain>max_info_gain:\n                        best_split[\"feature_index\"] = feature_index\n                        best_split[\"threshold\"] = threshold\n                        best_split[\"dataset_left\"] = dataset_left\n                        best_split[\"dataset_right\"] = dataset_right\n                        best_split[\"info_gain\"] = curr_info_gain\n                        max_info_gain = curr_info_gain\n                        \n        return best_split\n\n    def split(self, dataset, feature_index, threshold):\n        \"\"\"\n        Function to split the data to the left child and right child in the decision tree\n        dataset: input data\n        feature_index: feature index used to locate the index of the feature in a particular row in the dataset\n        threshold: threshold value based on which the split will be calculated\n        Returns the left and right datavalues from the dataset\n        \"\"\"\n        # Hint: Use list comprehension to distinguish which values would be present in left and right\n        # subtree on the basis of threshold\n        dataset_left = np.array([row for row in dataset if row[feature_index]<= threshold])\n        dataset_right = np.array([row for row in dataset if row[feature_index]> threshold])\n        return dataset_left, dataset_right\n\n    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n        \"\"\"\n        Function to calculate information gain. This function subtracts the combined information\n        of the child node from the parent node.\n        parent: value of parent node\n        l_child: value of left child node\n        r_child: value of right child node\n        mode: based on which information gain will be calculated either entropy/gini index\n        Returns the information gain\n        \"\"\"\n        \n        weight_l = len(l_child) / len(parent)\n        weight_r = len(r_child) / len(parent)\n        if mode==\"entropy\":\n            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n        else:\n            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n\n            \n        return gain\n\n    def entropy(self, y):\n        \"\"\"\n        Function to calculate the entropy\n        y: target labels\n        Returns entropy\n        \"\"\"\n        target_labels = np.unique(y)\n        entropy = 0\n        for label in target_labels:\n            p_label = len(y[y == label]) / len(y)\n            entropy += -p_label * np.log2(p_label)\n        return entropy\n\n    def gini_index(self, y):\n        \"\"\"\n        Function to calculate gini index\n        y: target labels\n        Returns gini index\n        \"\"\"\n        target_labels = np.unique(y)\n        gini = 0\n        for label in target_labels:\n            p_label = len(y[y == label]) / len(y)\n            gini += p_label**2\n        return 1 - gini\n\n    def calculate_leaf_value(self, Y):\n        \"\"\"\n        Function to compute the value of leaf node.Return the most occuring \n        element in Y. Hint: you can use lists\n        Y: target labels\n        Returns leaf node value\n        \"\"\"\n        # \n        Y = list(Y)\n        return max(Y, key=Y.count) \n\n    def print_tree(self, tree=None, indent=\" \"):\n        \"\"\"\n        Function to print the tree. Use the pre-order traversal method to print the decision tree.\n        # Do not make any changes in this function\n        \"\"\"\n\n        if not tree:\n            tree = self.root\n\n        if tree.value is not None:\n            print(tree.value)\n\n        else:\n            print(\"X \" + str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n            print(\"%sleft:\" % (indent), end=\" \")\n            self.print_tree(tree.left, indent + indent)\n            print(\"%sright\" % (indent), end=\" \")\n            self.print_tree(tree.right, indent + indent)\n\n    def fit(self, X, Y):\n        \"\"\"\n        Function to train the tree.\n        X: Features\n        Y: Target\n        \"\"\"\n        # Concatenate X, Y to create the dataset and call the build_tree function recursively\n        dataset = np.concatenate((X, Y), axis=1)\n        self.root = self.build_tree(dataset)\n\n    def predict(self, X):\n        \"\"\"\n        Prediction function to calculate the all the predictions of the matrix of features\n        provided using make_predictions function\n        X: Matrix of features\n        Returns predictions\n        \"\"\"\n        predictions = [self.make_predictions(x, self.root) for x in X]\n        return predictions\n\n    def make_predictions(self, x, tree):\n        \"\"\"\n        Function to predict a single datapoint\n        \"\"\"\n        # return the value if the node is a leaf node\n        if tree.value!=None: return tree.value\n        feature_val = x[tree.feature_index]\n        if feature_val<=tree.threshold:\n            return self.make_predictions(x, tree.left)\n        else:\n            return self.make_predictions(x, tree.right)\n        "
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "X 2 <= 1.9 ? 0.911553273406725\n left: 0.0\n right X 2 <= 4.7 ? 0.6373803478944154\n  left: X 0 <= 4.9 ? 0.205592508185083\n    left: 2.0\n    right 1.0\n  right X 2 <= 5.0 ? 0.19635725894022443\n    left: X 0 <= 6.3 ? 0.31668908831502096\n        left: 2.0\n        right 1.0\n    right 2.0\n"
                }
            ],
            "source": "#Splitting the data manually into 70% train, 30% test\nsplit = np.random.rand(len(data)) < 0.7\ntraining_data = data[split]\ntesting_data = data[~split]\n\nX_train = training_data[['feature1', 'feature2', 'feature3', 'feature4']].values\nX_test = testing_data[['feature1', 'feature2', 'feature3', 'feature4']].values\nY_train = training_data [\"class\"]\nY_test = testing_data[\"class\"]\n\n#Creating the \nscratch_classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n\ny = Y_train.to_numpy().reshape(-1,1)\n\n#Fit the training date\nscratch_classifier.fit(X_train,y)\n\n#Print the decisoon tree\nscratch_classifier.print_tree()"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[1 1 2 2 0 2 0 1 1 0 1 1 0 1 0 2 0 2 0 1 2 2 0 2 2 1 0 0 0 0 0 1 1 2 2 2 2\n 2 1 0 1 2 0 0 1 0 2 2 2]\n"
                }
            ],
            "source": "y_pred = scratch_classifier.predict(X_test)\ny_pred=np.array(y_pred).astype('int')\nprint(y_pred)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Test Accuracy: 95.9 %\n"
                }
            ],
            "source": "print('Test Accuracy: %.1f %%' % (np.mean(y_pred == Y_test) * 100))"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}